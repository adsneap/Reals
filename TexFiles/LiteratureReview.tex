\documentclass[ProjectReport]{subfiles}

\begin{document}

\section{Literature Review}
The mathematical framework within which to work is an important choice. Zermelo-Fraenkel Set Theory (ZFC) is dominant in the modern world, characterised by the axiom of choice. The axiom says that given a potentially infinite collection of inhabited sets, we can construct a new set by choosing one element from each set in the collection \cite{choiceaxiom}. The axiom gives rise to proofs that are otherwise not provable in the system. The choice axiom has been shown to be equivalent to the the theorem that every vector space has a basis. It is understandable why many mathematicians ware reluctant to give up this axiom; it provides many key results that are nice to have, so the thought of abandoning choice is not an appealing prospect. 

Constructive mathematics is a framework in which every proof provides a witness. This is clearly advantageous computationally, since constructive proofs provide an algorithm to obtain explicit witness of the proof. With a classical proof utilising the axiom of choice, it is not so simple, and producing an algorithm may be very difficult or even impossible. Mathematicians who prefer constructive mathematics may argue that the axiom should not be used, since it is not constructive, and what use is a proof if you cannot provide an example of its application? 

In some constructive frameworks, by adding the axiom of choice to we obtain classical mathematics. One might view constructive mathematics as embedded in classical mathematics, although this is not possible for some flavours of of constructive mathematics, for example Brouwer's Intuitionism. Adopting constructive mathematics has drawbacks. The proofs which are only true using the axiom of choice are lost. Usually, there are also constructive arguments which give the same proof, or a weaker version of the proof, but not always. Constructive arguments may turn out to be more complex. On the other hand, the HoTT Book \cite[Introduction - Constructivity]{hottbook} discusses how this is not usually a problem. Definitions can be adapted, theorems can be made constructed, and constructive variants of proofs usually shed light on or provide new insights on the crux of a proof.

Classical mathematics has had much more development, since constructive mathematics is a young field. There has been a lot of research in the area, since Brouwer begun pioneering intuitionism in 1907 \cite{Brouwer}. For example, Bishop made major contributions to constructive analysis \cite{Bishop1987-BISCA-2} followed by Troelstra \cite{Troelstra1973-TROMIO}, while progress is being made by multiple authors in constructive algebra \cite{2015}. 

Type theory is another young field living in the intersection of Mathematics, Computer Science and Logic. In the early 20th century, Russel introduced a system known as Type Theory to avoid the growing number of paradoxes in mathematics \cite{coquand_type_2018}. In the time since, there has been a huge amount of research in the field, and correspondences have been established between Type Theory, Mathematics and Logic. In 1980, Howard explicitly stated this correspondence \cite{Howard1980-HOWTFN-2}, which has since been extended to include category theory interpretations of Type Theory, and an even younger field known as Homotopy Type Theory (see \cite{hottbook}). 

One alternative foundation of mathematics is known as Intuitionistic Type Theory, or Martin-L\"{o}f Type Theory (MLTT) after its founder Per Martin L\"{o}f. This type theory is aiming to be to constructive mathematics what ZFC is to classical mathematics \cite{dybjer_intuitionistic_2020}. One model of MLTT is known as Univalent Type Theory, introduced by Voevodsky, which is characterised by the Univalence axiom. 

The goal of Univalent Type Theory is pure; as Grayson put it, Univalent foundations may provide ``a language for mathematics invariant under equivalence and thus freed from irrelevant details and able to merge the results of mathematicians taking different but equivalent approaches" \cite{grayson_introduction_2018}. The applications of such a foundation is clear; using proof assistants such as Agda and Coq to verify the \textit{correctness} of a proof eliminates this burden for mathematicians. Moreover, while the theory is borne out of constructive thinking, it is not limited to constructive mathematics. By adding certain axioms to the system, one can obtain classical mathematics, allowing one to verify the correctness of classical proofs. 

The power of this system goes even further when considering the consequence of writing propositions as types. By constructing an inhabitant of this type, we have a proof of this type. But writing this proof in a proof assistant allows us to extract a program to compute it, which is unquestionably useful to mathematicians: by writing proofs in such a language we can produce algorithms which are verifiably correct. Widely accepted incorrect proofs \cite{Incorrect_Proofs} are not unheard of.

In my view, there is no correct answer in choosing which mathematical foundation to work in. I can understand why a classical mathematician would be reluctant to give up the axiom of choice or the law of excluded middle, but also why a constructive mathematician champions the benefits of constructive proofs. The motivation behind a Univalent foundation of mathematics is exciting to me, I can envision a world where all mathematical proofs are readily available, regardless of foundational system, in verifiably correct libraries, and with algorithms readily available for constructive proofs. While it may be more difficult to write certain proofs constructively, I think it is a small price to pay, and a classical mathematician should have no worries in using a constructive version of a proof, and may continue to work classically by adding the axiom of choice to Univalent Foundations.

%There are developments of the Reals that exist, namely {TODO : ADD SOURCE} a Coq library by Andreij Bauer and a non-constructive Agda development.

My development will have exact computation of the reals up to an arbitrary error term. Therefore, I can guarantee the correctness of computation with the reals. This is an important feature, since mathematics drives the modern world, and numerical errors in calculation can have catastrophic consequences. In 1996, the Ariane 501 exploded at an estimated cost of \$300 million \cite{ARIANE1}, due to an overflow of floating point arithmetic. More examples can be found, the Patriot Rocket disaster for one, resulting in the loss of life of 28 individuals \cite{office_patriot_nodate}. Stevenson discusses in ``A critical look at quality in large-scale" simulations \cite{QualityInSimulation} how these problems arise, and how important validation is in the context of software. Also discussed is the dichotomy of the \textit{cost} of validation. 

Numerical analysis is the study of algorithms with approximate solutions to problems. To consider one example, I am studying numerical linear algebra as a module this year, and the two most important factors when considering algorithms is the time complexity of the algorithm, and the stability of the algorithm. If the algorithm is unstable, then the solution may be far away from the true solution, due to the buildup of errors in the floating point arithmetic during the algorithm. By using exact computation, we can avoid all instability, giving the answer to whatever degree of precision the user requires.

Exact computation is not free. As discussed in \cite{QualityInSimulation}, ``time is money", and there are many scenarios in which approximations are more suitable. Efficiency is important, but for applications such as software running in nuclear power plants, or involved in the design of the structures in the plant, safety-critical code is worth the price. The conversation of the correctness of code against efficiency is worth having on a per application basis, but in my view having the option of guaranteed correctness is a worthwhile investment.

With the above in mind, writing a constructive library for the Dedekind Reals is an exciting project for me. It poses various challenges, one being that I cannot simply copy and paste classical proofs from literature, and with a background in classical mathematics it forces me to think in a constructive way. I have to take care with definitions to ensure they hold constructively, and adapt proofs where necessary to avoid classical arguments. I have the added benefit in that my supervisor, Martin Escard\'o, has an active development of a library Univalent Type Theory \cite{TypeTopology}, and so my work will contribute to an active development in a young field at the intersection of Mathematics and Computer Science. 

Throughout this report, I will remark on proofs and definitions which are altered due to my choice to work constructively, perhaps with the hope to entice a reader to think in a more constructive way, or to do further reading in the field of constructive mathematics.

\end{document}